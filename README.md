<h4>1.全站爬虫：使用scrapy-crawl全站爬虫，爬取闲鱼二手交易网站所有二手物品发帖人以及发帖地址。</h4>
<h4>2.百万+ 数据</h4>
<h4>3.目的是为后期统计闲鱼地区活跃度，以及任务活跃度提供数据支持。</h4>
<h4>4.spider文件比较简单，因为是爬取非详情页信息，主要在于链接rule的提取，因为闲鱼链接真的好乱</h4>
<h4>5.直接写入mongodb数据库</h4>
<h4>6.自动更换请求头</h4>
<h4>7.自动限速</h4>
<h2>通过这些数据能做什么？</h2>
<h3>虽然只采集两个字段，一个是发帖人，一个是发帖地址，但是能做的多了去了，比如如果你是宣传闲鱼的策划，那你完全可以针对发帖活跃量少的地方进行大力宣传呀，而且如果您有其他信息的采集需要，只需添加简单的添加网址规则，以及要提取信息的提取方式即可，因为采用scrapy框架的自动限速以及自动更换请求头，在实际采集中，爬虫持续运行多个小时也没有被封禁</h3>
